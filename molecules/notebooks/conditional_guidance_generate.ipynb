{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import tqdm\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import dnnlib\n",
    "from pathlib import Path\n",
    "from torch_utils import distributed as dist\n",
    "from torch_utils.misc import modify_network_pkl\n",
    "from training.sampler import StackedRandomGenerator, samplers_to_kwargs\n",
    "from training.structure import Structure, StructuredDataBatch\n",
    "from training.networks.egnn import EGNNMultiHeadJump\n",
    "from training.loss import JumpLossFinalDim\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import yaml\n",
    "from training.dataset.qm9 import plot_data3d\n",
    "from training.dataset import datasets_to_kwargs\n",
    "import time\n",
    "\n",
    "model_path = Path('../models/unconditional')\n",
    "device = 'cuda'\n",
    "sampler_class = 'JumpSampler'\n",
    "\n",
    "sampler_kwargs = {\n",
    "    'dt': 0.001,\n",
    "    'corrector_steps': 3,\n",
    "    'corrector_snr': 0.1,\n",
    "    'corrector_start_time': 1,\n",
    "    'corrector_finish_time': 0.003,\n",
    "    'do_conditioning': True,\n",
    "    'condition_type': 'sweep',\n",
    "    'condition_sweep_idx': 9, # to decide which conditioning task to do 0-9\n",
    "    'condition_sweep_path': '../data/mol_conditions/mol_conds.npy',\n",
    "    'guidance_weight': 1.0,\n",
    "    'do_jump_corrector': True,\n",
    "    'sample_near_atom': True,\n",
    "    'dt_schedule': 'uniform',\n",
    "    'dt_schedule_h': 0.05,\n",
    "    'dt_schedule_l': 0.001,\n",
    "    'dt_schedule_tc': 0.5,\n",
    "    'no_noise_final_step': True,\n",
    "}\n",
    "\n",
    "with open(model_path.joinpath('training_options.json'), \"r\") as stream:\n",
    "    c = dnnlib.util.EasyDict(json.load(stream))\n",
    "\n",
    "def convert_inner_dicts_to_easydicts(input_dict):\n",
    "    for key in input_dict.keys():\n",
    "        if type(input_dict[key]) == dict:\n",
    "            input_dict[key] = convert_inner_dicts_to_easydicts(input_dict[key])\n",
    "    input_dict = dnnlib.util.EasyDict(input_dict)\n",
    "    return input_dict\n",
    "\n",
    "c = convert_inner_dicts_to_easydicts(c)\n",
    "\n",
    "dataset_obj = dnnlib.util.construct_class_by_name(**c.dataset_kwargs, train_or_valid='valid')  # subclass of training.dataset.Dataset\n",
    "\n",
    "structure = Structure(**c.structure_kwargs, dataset=dataset_obj)\n",
    "net = dnnlib.util.construct_class_by_name(**c.network_kwargs, structure=structure) # subclass of torch.nn.Module\n",
    "net.load_state_dict(torch.load(model_path.joinpath('state_dict_unconditional.pt')))\n",
    "net = net.eval().requires_grad_(False).to(device)\n",
    "# modify_network_pkl(net)\n",
    "\n",
    "# Setup sampler\n",
    "sampler_class_name = 'training.sampler.' + sampler_class\n",
    "usable_sampler_kwargs = dnnlib.EasyDict(class_name=sampler_class_name)\n",
    "for kwarg_name, _, _ in samplers_to_kwargs[sampler_class]:\n",
    "    # new_kwarg_name = \"_\".join(kwarg_name.split(\"_\")[1:])\n",
    "    usable_sampler_kwargs[kwarg_name] = sampler_kwargs[kwarg_name]\n",
    "sampler = dnnlib.util.construct_class_by_name(**usable_sampler_kwargs, structure=structure)\n",
    "\n",
    "# infer the task from the dataset\n",
    "dataset_class_name = c.dataset_kwargs['class_name'].split('training.dataset.')[1]\n",
    "if dataset_class_name not in ['QM9Dataset']:\n",
    "    raise ValueError('Unknown dataset: ', dataset_class_name)\n",
    "\n",
    "del(c.loss_kwargs['class_name'])\n",
    "loss = JumpLossFinalDim(**c.loss_kwargs, structure=structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seeds = torch.arange(batch_size)\n",
    "rnd = StackedRandomGenerator(device, seeds)\n",
    "indices = rnd.randint(len(dataset_obj), size=[batch_size, 1], device=device)\n",
    "unstacked_data = [dataset_obj.__getitem__(i.item(), will_augment=False) for i in indices]\n",
    "unstacked_data_no_dims = [d[1:] for d in unstacked_data]\n",
    "dims = torch.tensor([d[0] for d in unstacked_data])\n",
    "data = tuple(torch.stack([datum[t] for datum in unstacked_data_no_dims]).to(device) for t in range(len(unstacked_data_no_dims[0])))\n",
    "st_batch = StructuredDataBatch(data, dims, structure.observed,\n",
    "    structure.exist, dataset_obj.is_onehot, structure.graphical_structure\n",
    ")\n",
    "known_dims = None\n",
    "x0_st_batch = sampler.sample(net, st_batch, loss, rnd, known_dims=known_dims,\n",
    "                                dataset_obj=dataset_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_plot = min(batch_size, 8)\n",
    "for idx in range(num_to_plot):\n",
    "    num_atoms = x0_st_batch.get_dims()[idx].item()\n",
    "    positions = x0_st_batch.tuple_batch[0][idx, 0:num_atoms, :].cpu().detach()\n",
    "    atom_types = torch.argmax(x0_st_batch.tuple_batch[1][idx, 0:num_atoms, :], dim=1).cpu().detach()\n",
    "    plot_data3d(positions, atom_types, dataset_obj.dataset_info, spheres_3d=False)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
